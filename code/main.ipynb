{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d6d6b4-b49d-45b5-8cd6-31cd66ccf14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.1+cu121\n",
      "CUDA version: 12.1\n",
      "CUDNN version: 8902\n",
      "Number of CUDA devices: 1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "print(torch.cuda.is_available())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb398c56-046b-41e5-8bf7-0a2d4e44b068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
      "Requirement already satisfied: torch-scatter in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.2+pt22cu121)\n",
      "Requirement already satisfied: torch-sparse in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.6.18+pt22cu121)\n",
      "Requirement already satisfied: torch-cluster in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.6.3+pt22cu121)\n",
      "Requirement already satisfied: torch-spline-conv in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.2.2+pt22cu121)\n",
      "Requirement already satisfied: torch-geometric in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-sparse) (1.11.4)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (3.11.11)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (6.1.1)\n",
      "Requirement already satisfied: pyparsing in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch-geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch-geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch-geometric) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchvision in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.17.1+cu121)\n",
      "Requirement already satisfied: albumentations in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: kornia in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: matplotlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: plotly in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (6.0.0)\n",
      "Requirement already satisfied: torchinfo in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: open3d in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: tensorboard in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (2.2.1+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.6.85)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (1.11.4)\n",
      "Requirement already satisfied: PyYAML in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (2.10.5)\n",
      "Requirement already satisfied: albucore==0.0.23 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
      "Requirement already satisfied: kornia_rs>=0.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kornia) (0.1.8)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kornia) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (4.55.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from plotly) (1.24.1)\n",
      "Requirement already satisfied: dash>=2.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (2.18.2)\n",
      "Requirement already satisfied: werkzeug>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (3.0.6)\n",
      "Requirement already satisfied: flask>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (3.0.3)\n",
      "Requirement already satisfied: nbformat>=5.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (5.10.4)\n",
      "Requirement already satisfied: configargparse in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (1.7)\n",
      "Requirement already satisfied: ipywidgets>=8.0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (8.1.1)\n",
      "Requirement already satisfied: addict in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (2.4.0)\n",
      "Requirement already satisfied: pandas>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (4.67.1)\n",
      "Requirement already satisfied: pyquaternion in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from open3d) (0.9.9)\n",
      "Requirement already satisfied: absl-py>=0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: keras>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (5.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (8.6.1)\n",
      "Requirement already satisfied: retrying in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (1.3.4)\n",
      "Requirement already satisfied: nest-asyncio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (8.17.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.0->open3d) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.0->open3d) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n",
      "Requirement already satisfied: decorator in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.22.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
      "Requirement already satisfied: zipp>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ## Setup Environment\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
    "!pip install torchvision albumentations kornia matplotlib plotly torchinfo open3d tensorboard tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c706cbdb-adff-43b4-90d8-88e240693757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iopath in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.10)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath) (4.12.2)\n",
      "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath) (3.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt221/download.html\n",
      "Requirement already satisfied: pytorch3d in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.7.6)\n",
      "Requirement already satisfied: fvcore in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch3d) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch3d) (0.1.10)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (1.26.4)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (2.5.0)\n",
      "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (11.1.0)\n",
      "Requirement already satisfied: tabulate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath->pytorch3d) (4.12.2)\n",
      "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath->pytorch3d) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "#pytorch_3d\n",
    "pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "version_str = f\"py3{sys.version_info.minor}_cu{torch.version.cuda.replace('.','')}_pyt{pyt_version_str}\"\n",
    "\n",
    "!pip install iopath\n",
    "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9121e92-a37e-442b-ad4c-70ba990fdfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fvcore in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.10)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (1.26.4)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (2.5.0)\n",
      "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (11.1.0)\n",
      "Requirement already satisfied: tabulate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath) (4.12.2)\n",
      "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath) (3.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fvcore iopath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b8d701-2369-4df7-a167-0c688ef3be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt221/download.html\n",
      "Requirement already satisfied: pytorch3d in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.7.6)\n",
      "Requirement already satisfied: fvcore in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch3d) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch3d) (0.1.10)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (1.26.4)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (2.5.0)\n",
      "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (11.1.0)\n",
      "Requirement already satisfied: tabulate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fvcore->pytorch3d) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath->pytorch3d) (4.12.2)\n",
      "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from iopath->pytorch3d) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt221/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9659e9cb-6e92-4bcf-a681-4fabe6e35964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:23:59.039255: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-04 22:23:59.054520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738707839.074521   39558 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738707839.080632   39558 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 22:23:59.100354: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics import MeanAbsoluteError, MeanSquaredError  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import kornia as K\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import zipfile\n",
    "\n",
    "\n",
    "# Additional imports for point cloud processing and tensor batching.\n",
    "# (Ensure that these functions are installed and available in your environment.)\n",
    "from torch_geometric.nn import PointNetConv, fps, radius\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_scatter import scatter_mean, scatter_std\n",
    "from pytorch3d.ops import box3d_overlap\n",
    "\n",
    "\n",
    "# Get the directory where the notebook is located\n",
    "notebook_dir = os.getcwd()  \n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = os.path.join(notebook_dir, \"sample_dataset.zip\")\n",
    "\n",
    "# Extract the contents into the same directory as the notebook\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(notebook_dir)\n",
    "\n",
    "# Logging Setup\n",
    "log_file_path = os.path.join(os.getcwd(), 'logging.txt')\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n",
    "\n",
    "# Global Paths & Configuration\n",
    "BASE_PATH = os.path.join(os.getcwd(), 'dataset')\n",
    "LOG_DIR = os.path.join(os.getcwd(), 'logs')\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Global configuration settings.\n",
    "    \"\"\"\n",
    "    TRAIN_SPLIT = 0.7\n",
    "    VAL_SPLIT = 0.15\n",
    "    TEST_SPLIT = 0.15\n",
    "    RGB_SIZE = (128, 128)\n",
    "    LIDAR_POINTS = 1024\n",
    "    LIDAR_ROT_RANGE = (-15, 15)  # degrees\n",
    "    LIDAR_SCALE_RANGE = (0.9, 1.1)\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 2\n",
    "    SEED = 42\n",
    "    NUM_EPOCHS = 50\n",
    "    DEBUG_MODE = False  # Set to True to enable single-sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77dc4f46-539f-467f-91e4-4ab86ca1a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging & Visualization Setup\n",
    "class DebugVisualizer:\n",
    "    \"\"\"\n",
    "    Provides simple logging and visualization utilities.\n",
    "    All outputs are saved to LOG_DIR.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.log_dir = LOG_DIR\n",
    "        self.image_dir = os.path.join(self.log_dir, 'images')\n",
    "        self.pointcloud_dir = os.path.join(self.log_dir, 'pointclouds')\n",
    "        self.distribution_dir = os.path.join(self.log_dir, 'distributions')\n",
    "\n",
    "        os.makedirs(self.image_dir, exist_ok=True)\n",
    "        os.makedirs(self.pointcloud_dir, exist_ok=True)\n",
    "\n",
    "    def denormalize(self, tensor):\n",
    "        \"\"\"Convert normalized image tensor to displayable format. IMAGENET\"\"\"\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "        tensor = tensor * std + mean\n",
    "        return tensor\n",
    "\n",
    "    def log_image(self, image, bbox, title):\n",
    "        \"\"\"\n",
    "        Save an RGB image to file with an optional bounding box.\n",
    "        \"\"\"\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.cpu().detach()\n",
    "            if image.dim() == 4 and image.size(0) == 1:\n",
    "                image = image.squeeze(0)\n",
    "            # Denormalize the image to visualize\n",
    "            image = self.denormalize(image)\n",
    "            image = image.numpy().transpose(1, 2, 0)\n",
    "\n",
    "        image = np.clip(image, 0, 1)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(image)\n",
    "        if bbox is not None:\n",
    "            rect = plt.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2] - bbox[0],\n",
    "                                 bbox[3] - bbox[1],\n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        plt.title(title)\n",
    "        image_path = os.path.join(self.image_dir, f\"{title}.png\")\n",
    "        plt.savefig(image_path)\n",
    "        plt.close()\n",
    "\n",
    "    def log_pointcloud(self, pc, title='pointcloud', include_plotlyjs='cdn'):\n",
    "        \"\"\"\n",
    "        Save a 3D point cloud visualization as an HTML file.\n",
    "        \"\"\"\n",
    "        fig = go.Figure(data=[go.Scatter3d(\n",
    "            x=pc[:, 0],\n",
    "            y=pc[:, 1],\n",
    "            z=pc[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=2, color=pc[:, 2])\n",
    "        )])\n",
    "        pointcloud_path = os.path.join(self.pointcloud_dir, f\"{title}.html\")\n",
    "        fig.show()\n",
    "        fig.write_html(pointcloud_path, include_plotlyjs=include_plotlyjs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69431632-8de6-44bb-add1-44b23bf1007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "class Augmentor:\n",
    "    \"\"\"\n",
    "    Performs data augmentation for RGB and LiDAR data.\n",
    "    \"\"\"\n",
    "    def __init__(self, split):\n",
    "        self.split = split\n",
    "        self.rgb_transform = self._get_rgb_transforms()\n",
    "        self.lidar_transform = self._get_lidar_transforms()\n",
    "\n",
    "    def _get_rgb_transforms(self):\n",
    "        transforms = [\n",
    "            A.Resize(*Config.RGB_SIZE),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406],  # normalization with imagenet mean/std\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ]\n",
    "        if self.split == 'train':\n",
    "            transforms.insert(1, A.HorizontalFlip(p=0.5))\n",
    "            transforms.insert(1, A.ColorJitter(\n",
    "                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.7\n",
    "            ))\n",
    "        return A.Compose(transforms)\n",
    "\n",
    "    def _get_lidar_transforms(self):\n",
    "        if self.split != 'train':\n",
    "            return None\n",
    "        return nn.Sequential(\n",
    "            K.augmentation.RandomRotation3D(degrees=Config.LIDAR_ROT_RANGE, p=0.7),\n",
    "            K.augmentation.RandomAffine3D(degrees=Config.LIDAR_ROT_RANGE,\n",
    "                                            scale=Config.LIDAR_SCALE_RANGE, p=0.5)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655d1c3e-bccf-47a1-a9c2-ae26c66d3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & Preprocessing\n",
    "class FrustumDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses scene data for 3D bounding box prediction.\n",
    "    Each scene includes an RGB image, point cloud, mask, and 3D bbox.\n",
    "    \"\"\"\n",
    "    def __init__(self, scene_paths, split='train'):\n",
    "        self.scene_paths = scene_paths\n",
    "        self.split = split\n",
    "        self.augmentor = Augmentor(split)\n",
    "        self.instances = []\n",
    "        for path in tqdm(scene_paths, desc=\"Processing Scenes\"):\n",
    "            # Check if required files exist for the scene.\n",
    "            if all([(path / 'rgb.jpg').exists(),\n",
    "                    (path / 'pc.npy').exists(),\n",
    "                    (path / 'mask.npy').exists()]):\n",
    "                scene_data = self._load_scene(path)\n",
    "                for instance in scene_data['instances']:\n",
    "                    self.instances.append({\n",
    "                        'scene_id': scene_data['scene_id'],\n",
    "                        'instance': instance\n",
    "                    })\n",
    "            else:\n",
    "                logging.warning(f\"Invalid scene: {path.name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance_data = self.instances[idx]\n",
    "        # If debug mode is enabled, visualize a single random instance.\n",
    "        if Config.DEBUG_MODE and random.random() < 0.01:\n",
    "            self._debug_visualization(instance_data)\n",
    "        return instance_data\n",
    "\n",
    "    def _load_scene(self, path):\n",
    "        try:\n",
    "            rgb = cv2.cvtColor(cv2.imread(str(path / 'rgb.jpg')), cv2.COLOR_BGR2RGB)\n",
    "            pc = np.load(path / 'pc.npy')\n",
    "            masks = np.load(path / 'mask.npy')\n",
    "            bboxes = np.load(path / 'bbox3d.npy') if (path / 'bbox3d.npy').exists() else None\n",
    "\n",
    "            instances = []\n",
    "            for i in range(masks.shape[0]):\n",
    "                instance = self._process_instance(rgb, pc, masks[i], bboxes[i] if bboxes is not None else None)\n",
    "                if instance is not None:\n",
    "                    instances.append(instance)\n",
    "\n",
    "            return {\n",
    "                'scene_id': path.name,\n",
    "                'instances': instances,\n",
    "                'num_instances': len(instances)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading scene {path.name}: {str(e)}\")\n",
    "            return {'scene_id': path.name, 'instances': [], 'num_instances': 0}\n",
    "\n",
    "    def _process_instance(self, rgb, pc, mask, bbox):\n",
    "        try:\n",
    "            if np.isnan(rgb).any() or np.isnan(pc).any() or np.isnan(mask).any():\n",
    "                logging.warning(\"NaN detected in input data!\")\n",
    "                return None\n",
    "            if np.isinf(rgb).any() or np.isinf(pc).any() or np.isinf(mask).any():\n",
    "                logging.warning(\"Inf detected in input data!\")\n",
    "                return None\n",
    "\n",
    "            if np.sum(mask) < 1:\n",
    "                logging.warning(\"Empty mask detected in input data!\")\n",
    "                return None\n",
    "\n",
    "            bbox_2d = self._get_bbox_from_mask(mask)\n",
    "            if bbox_2d is None:\n",
    "                return None\n",
    "\n",
    "            xmin, ymin, xmax, ymax = bbox_2d\n",
    "            cropped_rgb = rgb[ymin:ymax, xmin:xmax]\n",
    "            transformed = self.augmentor.rgb_transform(image=cropped_rgb)\n",
    "            rgb_crop = transformed['image']\n",
    "\n",
    "            if len(rgb_crop.shape) != 3:\n",
    "                logging.error(f\"Inconsistent RGB shape: {rgb_crop.shape}\")\n",
    "                return None\n",
    "\n",
    "            pc_tensor, centroid = self._process_lidar(pc, mask)\n",
    "            bbox_data = self._process_bbox(bbox) if bbox is not None else None\n",
    "\n",
    "            return {\n",
    "                'rgb': rgb_crop,\n",
    "                'pc': pc_tensor,\n",
    "                'centroid': centroid,\n",
    "                'bbox': bbox_data\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Instance processing failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _process_lidar(self, pc, mask):\n",
    "        \"\"\"\n",
    "        Extract LiDAR points from the point cloud using the mask and ensure a fixed number of points.\n",
    "        \"\"\"\n",
    "        pc_reshaped = np.transpose(pc, (1, 2, 0))\n",
    "        if mask.shape != pc_reshaped.shape[:2]:\n",
    "            raise ValueError(f\"Mask shape {mask.shape} does not match LiDAR dimensions {pc_reshaped.shape[:2]}\")\n",
    "        processed_points = pc_reshaped[mask.astype(bool)]\n",
    "        if processed_points.shape[0] == 0:\n",
    "            raise ValueError(\"No LiDAR points extracted from mask!\")\n",
    "        centroid = np.mean(processed_points, axis=0)\n",
    "        pc_tensor = torch.tensor(processed_points, dtype=torch.float32)\n",
    "        #reshape to meet kornia transforms\n",
    "        if self.augmentor.lidar_transform:\n",
    "            augmented = self.augmentor.lidar_transform(pc_tensor.unsqueeze(0))\n",
    "            while augmented.dim() > 2 and augmented.size(0) == 1:\n",
    "                augmented = augmented.squeeze(0)\n",
    "            if augmented.dim() != 2 or augmented.size(1) != 3:\n",
    "                raise ValueError(f\"After augmentation, expected shape (N,3) but got {augmented.shape}\")\n",
    "            pc_tensor = augmented\n",
    "        if pc_tensor.shape[0] < Config.LIDAR_POINTS:\n",
    "            padding = torch.zeros(Config.LIDAR_POINTS - pc_tensor.shape[0], 3)\n",
    "            pc_tensor = torch.cat([pc_tensor, padding], dim=0)\n",
    "        else:\n",
    "            idx = torch.randperm(pc_tensor.shape[0])[:Config.LIDAR_POINTS]\n",
    "            pc_tensor = pc_tensor[idx]\n",
    "        return pc_tensor, torch.tensor(centroid, dtype=torch.float32)\n",
    "\n",
    "    def _get_bbox_from_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Extract a 2D bounding box from a binary mask.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mask_uint8 = mask.astype(np.uint8)\n",
    "            rows = np.any(mask_uint8, axis=1)\n",
    "            cols = np.any(mask_uint8, axis=0)\n",
    "            ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "            xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "            if (xmax - xmin) < 2 or (ymax - ymin) < 2:\n",
    "                return None\n",
    "            return (xmin, ymin, xmax, ymax)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"BBox extraction failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _process_bbox(self, bbox, num_bins=2):\n",
    "        \"\"\"\n",
    "        Process a raw 3D bounding box (8,3) into target parameters:\n",
    "          - center, dimensions, orientation class and residual.\n",
    "        \"\"\"\n",
    "        if bbox.shape != (8, 3):\n",
    "            raise ValueError(f\"Expected bbox shape (8,3), got {bbox.shape}\")\n",
    "        center = np.mean(bbox, axis=0)\n",
    "        dimensions = np.max(bbox, axis=0) - np.min(bbox, axis=0)\n",
    "        delta = bbox[1] - bbox[0]\n",
    "        yaw = np.arctan2(delta[1], delta[0])\n",
    "        # Discretizing the continuous yaw angle into one of several bins.\n",
    "        bin_size = 2 * np.pi / num_bins\n",
    "        yaw_shifted = yaw + np.pi\n",
    "        bin_idx = int(np.floor(yaw_shifted / bin_size))\n",
    "        bin_idx = min(bin_idx, num_bins - 1)\n",
    "        bin_center = bin_idx * bin_size + bin_size / 2.0 - np.pi\n",
    "        residual = yaw - bin_center\n",
    "        return {\n",
    "            'center': torch.tensor(center, dtype=torch.float32),\n",
    "            'dims': torch.tensor(dimensions, dtype=torch.float32),\n",
    "            'orient_cls': torch.tensor(bin_idx, dtype=torch.long),\n",
    "            'orient_reg': torch.tensor(residual, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "    def _debug_visualization(self, instance_data):\n",
    "        \"\"\"\n",
    "        Visualize a single preprocessed sample:\n",
    "          - Log the augmented RGB image with its 2D bounding box overlay.\n",
    "          - Log the masked 3D point cloud.\n",
    "        This is only invoked if Config.DEBUG_MODE is True.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            instance = instance_data['instance']\n",
    "            scene_id = instance_data['scene_id']\n",
    "            # Augmented RGB image\n",
    "            rgb_tensor = instance['rgb']\n",
    "            rgb_np = rgb_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "            bbox_2d = instance.get('bbox_2d', None)\n",
    "            debugger.log_image(rgb_np, bbox_2d, f\"{scene_id}_augmented_rgb_debug\")\n",
    "            # Masked 3D point cloud visualization\n",
    "            pc = instance['pc'].cpu().numpy()\n",
    "            debugger.log_pointcloud(pc, f\"{scene_id}_pc_debug\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Visualization failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0ff7db-58d8-43f3-8ab7-03cb5eaef456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader and Collation\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to merge individual instances into a batch.\n",
    "    \"\"\"\n",
    "    collated = {\n",
    "        'scene_ids': [], # for later scene level instance grouping at post-processing\n",
    "        'rgb': [],\n",
    "        'pc': [],\n",
    "        'centroid': [], # to scale back pc points\n",
    "        'bbox': {'center': [], 'dims': [], 'orient_cls': [], 'orient_reg': []}\n",
    "    }\n",
    "    for data in batch:\n",
    "        instance = data['instance']\n",
    "        collated['scene_ids'].append(data['scene_id'])\n",
    "        collated['rgb'].append(instance['rgb'])\n",
    "        collated['pc'].append(instance['pc'])\n",
    "        collated['centroid'].append(instance['centroid'])\n",
    "        bbox = instance.get('bbox', None)\n",
    "        if bbox is None:\n",
    "            continue\n",
    "        collated['bbox']['center'].append(bbox['center'])\n",
    "        collated['bbox']['dims'].append(bbox['dims'])\n",
    "        collated['bbox']['orient_cls'].append(bbox['orient_cls'])\n",
    "        collated['bbox']['orient_reg'].append(bbox['orient_reg'])\n",
    "    collated['rgb'] = torch.stack(collated['rgb'])\n",
    "    collated['pc'] = torch.stack(collated['pc'])\n",
    "    collated['bbox']['center'] = torch.stack(collated['bbox']['center'])\n",
    "    collated['bbox']['dims'] = torch.stack(collated['bbox']['dims'])\n",
    "    collated['bbox']['orient_cls'] = torch.stack(collated['bbox']['orient_cls'])\n",
    "    collated['bbox']['orient_reg'] = torch.stack(collated['bbox']['orient_reg'])\n",
    "    return collated\n",
    "\n",
    "def get_loaders():\n",
    "    \"\"\"\n",
    "    Create and return DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    dataset_path = BASE_PATH\n",
    "    all_scenes = list(Path(dataset_path).glob('*'))\n",
    "    logging.info(f\"Total scenes detected: {len(all_scenes)}\")\n",
    "    train_scenes, test_val_scenes = train_test_split(\n",
    "        all_scenes, test_size=1-Config.TRAIN_SPLIT, random_state=Config.SEED\n",
    "    )\n",
    "    val_scenes, test_scenes = train_test_split(\n",
    "        test_val_scenes, test_size=0.5, random_state=Config.SEED\n",
    "    )\n",
    "    #Dataset creation\n",
    "    train_dataset = FrustumDataset(train_scenes, 'train')\n",
    "    val_dataset = FrustumDataset(val_scenes, 'val')\n",
    "    test_dataset = FrustumDataset(test_scenes, 'test')\n",
    "    #Dataset loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=Config.NUM_WORKERS,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        ),\n",
    "        'val': DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            num_workers=Config.NUM_WORKERS,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        ),\n",
    "        'test': DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            num_workers=Config.NUM_WORKERS,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    }\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4855730e-4a16-4fbb-b18d-122dfb166250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Components\n",
    "class SimplePointNetPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified PointNet++ module for processing LiDAR point clouds.\n",
    "    ## refernce link: https://pytorch-geometric.readthedocs.io/en/2.5.1/tutorial/point_cloud.html\n",
    "    imported modules: PointNetConv,fps,radius\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = PointNetConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(3 + 3, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            add_self_loops=False\n",
    "        )\n",
    "        self.conv2 = PointNetConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(128 + 3, 256),  \n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 512),  \n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            add_self_loops=False\n",
    "        )\n",
    "    # Normalizing the lidar points batch-wise (instead during pre-processing)\n",
    "    def normalize_pos(self, pos, batch):\n",
    "        batch = batch.to(pos.device)\n",
    "        batch = batch - batch.min()\n",
    "        batch_size = batch.max().item() + 1\n",
    "        mean = scatter_mean(pos, batch, dim=0, dim_size=batch_size).to(pos.device)\n",
    "        std = scatter_std(pos, batch, dim=0, dim_size=batch_size).to(pos.device) + 1e-6\n",
    "        pos_norm = (pos - mean[batch]) / std[batch]\n",
    "        return pos_norm, batch\n",
    "\n",
    "    def forward(self, pos, batch):\n",
    "        pos, batch = self.normalize_pos(pos, batch)\n",
    "        # stage-1: sampling & grouping, pointnet\n",
    "        idx1 = fps(pos, batch, ratio=0.5)\n",
    "        pos1 = pos[idx1]\n",
    "        batch1 = batch[idx1]\n",
    "        edge_index1 = radius(pos1, pos1, r=0.1, batch_x=batch1, batch_y=batch1, max_num_neighbors=64) #hyperparam: max_num_neighbors, radii\n",
    "        x1 = self.conv1(x=pos1, pos=pos1, edge_index=edge_index1)\n",
    "        # stage-2: sampling & grouping, pointnet\n",
    "        idx2 = fps(pos1, batch1, ratio=0.5)\n",
    "        pos2 = pos1[idx2]\n",
    "        batch2 = batch1[idx2]\n",
    "        edge_index2 = radius(pos2, pos2, r=0.25, batch_x=batch2, batch_y=batch2, max_num_neighbors=64)\n",
    "        x2_input = x1[idx2]\n",
    "        x2 = self.conv2(x=x2_input, pos=pos2, edge_index=edge_index2)\n",
    "        return x2, batch2\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-18 based backbone for image feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze=True):\n",
    "        super().__init__()\n",
    "        base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.stem = nn.Sequential(\n",
    "            base_model.conv1, base_model.bn1, base_model.relu, base_model.maxpool\n",
    "        )\n",
    "        self.layer1 = base_model.layer1\n",
    "        self.layer2 = base_model.layer2\n",
    "        self.layer3 = base_model.layer3\n",
    "        self.layer4 = base_model.layer4\n",
    "        #Transfer learning\n",
    "        if freeze:\n",
    "            for param in self.stem.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layer1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layer2.parameters(): \n",
    "                param.requires_grad = False\n",
    "            for param in self.layer3.parameters(): \n",
    "                param.requires_grad = False\n",
    "            for param in self.layer4.parameters(): \n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return self.layer4(x)\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses LiDAR and image features using cross-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, lidar_dim=512, image_dim=512):\n",
    "        super().__init__()\n",
    "        self.image_proj = nn.Linear(image_dim, lidar_dim)\n",
    "        self.attention = nn.MultiheadAttention(lidar_dim, 4, batch_first=True)\n",
    "\n",
    "    def forward(self, lidar_feats, image_feats, mask=None):\n",
    "        image_feats = self.image_proj(image_feats)\n",
    "        attn_out, _ = self.attention(\n",
    "            query=lidar_feats,\n",
    "            key=image_feats,\n",
    "            value=image_feats,\n",
    "            key_padding_mask=mask\n",
    "        )\n",
    "        return attn_out\n",
    "\n",
    "# Complete model archietcture\n",
    "    \n",
    "class MultiModalBBoxPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end model that fuses image and LiDAR features to predict 3D bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_bins=2):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_bins = num_bins\n",
    "        self.pointnet = SimplePointNetPP()\n",
    "        self.resnet = ResNetBackbone(freeze=True)\n",
    "        self.img_norm = nn.LayerNorm(512)\n",
    "        self.point_norm = nn.LayerNorm(512)\n",
    "        self.fusion = CrossAttentionFusion(lidar_dim=512, image_dim=512)\n",
    "        #prediction heads\n",
    "        self.head_center = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "        self.head_dims = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.head_orient_cls = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128, self.num_bins)\n",
    "        )\n",
    "        self.head_orient_reg = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128, self.num_bins)\n",
    "        )\n",
    "        self.scale_factor_dims = nn.Parameter(torch.tensor(1.0))\n",
    "        self.global_pool_proj = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, rgb, lidar):\n",
    "        rgb = rgb.to(self.device)\n",
    "        lidar = lidar.to(self.device)\n",
    "        # Image pathway\n",
    "        img_feats = self.resnet(rgb)\n",
    "        b, c, h, w = img_feats.shape\n",
    "        img_feats = img_feats.view(b, c, -1).permute(0, 2, 1)\n",
    "        img_feats = self.img_norm(img_feats)\n",
    "        # LiDAR pathway\n",
    "        batch_size = rgb.shape[0]\n",
    "        num_points = lidar.shape[1]\n",
    "        pos = lidar.reshape(-1, 3).to(self.device)\n",
    "        # Batch processing\n",
    "        # Create a batch index tensor that assigns each point to its corresponding batch\n",
    "        # This is done by repeating each batch index 'num_points' times\n",
    "        batch = torch.arange(batch_size, device=self.device).repeat_interleave(num_points)\n",
    "\n",
    "        # Pass the positional data and batch indices through the PointNet module\n",
    "        # 'lidar_feats' contains the extracted features for each point\n",
    "        # 'lidar_batch' contains the batch indices corresponding to each feature\n",
    "        lidar_feats, lidar_batch = self.pointnet(pos, batch)\n",
    "\n",
    "        # Convert the sparse batch of point features into a dense tensor and reshaping with corresponding Batch dimensions\n",
    "        # 'point_feats_dense' has the shape [batch_size, max_num_nodes, feature_dim]\n",
    "        # 'mask' is a boolean tensor indicating the presence of valid nodes\n",
    "        point_feats_dense, mask = to_dense_batch(lidar_feats, lidar_batch, max_num_nodes=256, fill_value=0.0)\n",
    "        point_feats_dense = self.point_norm(point_feats_dense)\n",
    "        # Fusion via cross-attention\n",
    "        fused_dense = self.fusion(point_feats_dense, img_feats)\n",
    "        # Global pooling - max+avg pooling to preserve sparse features\n",
    "        fused_max = fused_dense.max(dim=1)[0]\n",
    "        fused_avg = fused_dense.mean(dim=1)\n",
    "        fused_global = self.global_pool_proj(torch.cat([fused_max, fused_avg], dim=1))\n",
    "        # Prediction heads\n",
    "        pred_center = self.head_center(fused_global)\n",
    "        pred_dims_raw = self.head_dims(fused_global)\n",
    "        pred_dims = pred_dims_raw * self.scale_factor_dims #predicted dimensions are too low relative to the ground truth,so defined a learnable dim scaling factor\n",
    "        pred_orient_cls = self.head_orient_cls(fused_global)\n",
    "        pred_orient_reg = self.head_orient_reg(fused_global)\n",
    "        return {\n",
    "            'center': pred_center,\n",
    "            'dims': pred_dims,\n",
    "            'orient_cls': pred_orient_cls,\n",
    "            'orient_reg': pred_orient_reg\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77fd2a2f-fb7d-40e6-9dc3-ba766b433976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Infrastructure\n",
    "class BBoxTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for the 3D bounding box prediction model.\n",
    "    Handles training, validation, and testing loops.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, patience = 4):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.writer = SummaryWriter()\n",
    "        self.optimizer = self.configure_optimizer()\n",
    "        # Using ReduceLROnPlateau scheduler to dynamically reduce the LR when validation loss plateaus.\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "        self.loss_weights = {'center': 1.0, 'dims': 1.0, 'yaw': 2.0}\n",
    "        self.patience = patience  # Early stopping patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "    def configure_optimizer(self):\n",
    "        params = [\n",
    "            {'params': self.model.resnet.layer3.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.model.resnet.layer4.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.model.pointnet.parameters(), 'lr': 5e-4},\n",
    "            {'params': self.model.fusion.parameters(), 'lr': 5e-4},\n",
    "            {'params': self.model.head_center.parameters(), 'lr': 5e-4},\n",
    "            {'params': self.model.head_dims.parameters(), 'lr': 5e-4},\n",
    "            {'params': self.model.head_orient_cls.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.model.head_orient_reg.parameters(), 'lr': 1e-4},\n",
    "        ]\n",
    "        return torch.optim.AdamW(params, lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "    #orientation loss compute\n",
    "    def yaw_loss_multibin(self, pred_cls, pred_reg, gt_cls, gt_reg):\n",
    "        loss_cls = F.cross_entropy(pred_cls, gt_cls)\n",
    "        pred_residual = pred_reg.gather(1, gt_cls.unsqueeze(1)).squeeze(1)\n",
    "        loss_reg = F.smooth_l1_loss(pred_residual, gt_reg)\n",
    "        return loss_cls + loss_reg\n",
    "    \n",
    "    #total loss compute\n",
    "    def compute_loss(self, preds, targets):\n",
    "        center_loss = F.smooth_l1_loss(preds['center'], targets['center'])\n",
    "        dims_loss = F.smooth_l1_loss(preds['dims'], targets['dims'])\n",
    "        loss_yaw = self.yaw_loss_multibin(preds['orient_cls'], preds['orient_reg'], targets['orient_cls'], targets['orient_reg'])\n",
    "        total_loss = (self.loss_weights['center'] * center_loss +\n",
    "                      self.loss_weights['dims'] * dims_loss +\n",
    "                      self.loss_weights['yaw'] * loss_yaw)\n",
    "        return total_loss\n",
    "\n",
    "    #training loop\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        avg_loss = 0\n",
    "        for batch in self.train_loader:\n",
    "            rgb = batch['rgb'].to(self.device, non_blocking=True)\n",
    "            pc = batch['pc'].to(self.device, non_blocking=True)\n",
    "            targets = {\n",
    "                'center': batch['bbox']['center'].to(self.device),\n",
    "                'dims': batch['bbox']['dims'].to(self.device),\n",
    "                'orient_cls': batch['bbox']['orient_cls'].to(self.device),\n",
    "                'orient_reg': batch['bbox']['orient_reg'].to(self.device)\n",
    "            }\n",
    "            # Reset gradients to avoid accumulation from previous steps\n",
    "            self.optimizer.zero_grad()\n",
    "            # Forward pass: Predict bounding boxes using the model\n",
    "            preds = self.model(rgb, pc)\n",
    "             # Compute the loss by comparing predictions with ground truth labels\n",
    "            loss = self.compute_loss(preds, targets)\n",
    "            # Backpropagation: Compute gradients of the loss w.r.t. model parameters\n",
    "            loss.backward()\n",
    "             # Gradient Clipping: Prevents exploding gradients by regulating their norm\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            # Update model parameters using the optimizer\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "        return avg_loss / len(self.train_loader)\n",
    "\n",
    "    #validation loop\n",
    "    def validate(self, epoch):\n",
    "        metrics = BBoxMetrics(stage='val', num_bins=self.model.num_bins)\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                rgb = batch['rgb'].to(self.device)\n",
    "                pc = batch['pc'].to(self.device)\n",
    "                targets = {\n",
    "                    'center': batch['bbox']['center'].to(self.device),\n",
    "                    'dims': batch['bbox']['dims'].to(self.device),\n",
    "                    'orient_cls': batch['bbox']['orient_cls'].to(self.device),\n",
    "                    'orient_reg': batch['bbox']['orient_reg'].to(self.device)\n",
    "                }\n",
    "                preds = self.model(rgb, pc)\n",
    "                loss = self.compute_loss(preds, targets)\n",
    "                val_loss += loss.item()\n",
    "                metrics.update(preds, targets, batch_size=rgb.size(0))\n",
    "        avg_val_loss = val_loss / len(self.val_loader)\n",
    "        metric_values = metrics.get_metrics()\n",
    "        self.log_metrics(metric_values, epoch, 'val')\n",
    "        # Log validation loss and metrics\n",
    "        logging.info(f\"Validation Metrics: {metric_values}\")\n",
    "        return avg_val_loss\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss = self.validate(epoch)\n",
    "            # Update the LR scheduler based on validation loss.\n",
    "            self.scheduler.step(val_loss)\n",
    "            logging.info(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            # Early stopping check.\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.epochs_no_improve = 0\n",
    "                torch.save(self.model.state_dict(), \"best_model.pth\")\n",
    "                logging.info(f\"New best model saved at epoch {epoch} with Val Loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                logging.info(f\"No improvement for {self.epochs_no_improve} epochs.\")\n",
    "                if self.epochs_no_improve >= self.patience:\n",
    "                    logging.info(\"Early stopping triggered.\")\n",
    "                    break\n",
    "    # evaluation loop\n",
    "    def evaluate_metrics(self, loader, stage):\n",
    "        \"\"\"\n",
    "        Evaluate detailed metrics on the given DataLoader using BBoxMetrics.\n",
    "        \"\"\"\n",
    "        metrics = BBoxMetrics(stage='test', num_bins=self.model.num_bins)\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                rgb = batch['rgb'].to(self.device)\n",
    "                pc = batch['pc'].to(self.device)\n",
    "                targets = {\n",
    "                    'center': batch['bbox']['center'].to(self.device),\n",
    "                    'dims': batch['bbox']['dims'].to(self.device),\n",
    "                    'orient_cls': batch['bbox']['orient_cls'].to(self.device),\n",
    "                    'orient_reg': batch['bbox']['orient_reg'].to(self.device)\n",
    "                }\n",
    "                preds = self.model(rgb, pc)\n",
    "                metrics.update(preds, targets, batch_size=rgb.size(0))\n",
    "                loss = self.compute_loss(preds, targets)\n",
    "                test_loss += loss.item()\n",
    "        avg_test_loss = test_loss / len(self.test_loader)\n",
    "        metric_values = metrics.get_metrics()\n",
    "        self.log_metrics(metric_values, -1, 'test')\n",
    "        logging.info(f\"{stage} Metrics: {metric_values}\")\n",
    "        logging.info(f\"{stage} Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "        return metric_values, avg_test_loss\n",
    "\n",
    "    def log_metrics(self, metrics, epoch, stage):\n",
    "        \"\"\"Log metrics to TensorBoard\"\"\"\n",
    "        for k, v in metrics.items():\n",
    "            self.writer.add_scalar(f'{stage}/{k}', v, int(epoch) if isinstance(epoch, int) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d10b2c3-a176-4c44-92f6-cf16189d638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBoxMetrics for Detailed Evaluation\n",
    "class BBoxMetrics:\n",
    "    \"\"\"\n",
    "    Metrics for 3D bounding boxes, adapted for multi-bin orientation.\n",
    "    \"\"\"\n",
    "    def __init__(self, stage='val', num_bins=2):\n",
    "        self.stage = stage\n",
    "        self.num_bins = num_bins\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mae_center = MeanAbsoluteError().to(self.device)\n",
    "        self.mse_center = MeanSquaredError().to(self.device)\n",
    "        self.mae_dims = MeanAbsoluteError().to(self.device)\n",
    "        self.mse_dims = MeanSquaredError().to(self.device)\n",
    "        self.angle_error = MeanAbsoluteError().to(self.device)\n",
    "        self.iou_3d = MeanAbsoluteError().to(self.device)  \n",
    "\n",
    "    def update(self, preds, targets, batch_size):\n",
    "        # Center metrics\n",
    "        self.mae_center(preds['center'], targets['center'])\n",
    "        self.mse_center(preds['center'], targets['center'])\n",
    "        # Dimension metrics\n",
    "        self.mae_dims(preds['dims'], targets['dims'])\n",
    "        self.mse_dims(preds['dims'], targets['dims'])\n",
    "        # Orientation error\n",
    "        num_bins = self.num_bins\n",
    "        bin_size = 2 * math.pi / num_bins\n",
    "        pred_yaw_angles = multibin_to_yaw(preds['orient_cls'], preds['orient_reg'], num_bins)\n",
    "        gt_yaw_angles = targets['orient_reg'] + (targets['orient_cls'].float() * bin_size + bin_size / 2.0 - math.pi)\n",
    "        # Compute angular difference \n",
    "        angle_diff = torch.remainder(pred_yaw_angles - gt_yaw_angles + math.pi, 2 * math.pi) - math.pi\n",
    "        angle_diff_deg = torch.abs(torch.rad2deg(angle_diff))\n",
    "        self.angle_error(angle_diff_deg, torch.zeros_like(angle_diff_deg))\n",
    "        \n",
    "        # Convert yaw angles to (sin, cos) vectors\n",
    "        pred_yaw_vec = torch.stack([torch.sin(pred_yaw_angles), torch.cos(pred_yaw_angles)], dim=1)\n",
    "        gt_yaw_vec = torch.stack([torch.sin(gt_yaw_angles), torch.cos(gt_yaw_angles)], dim=1)\n",
    "        \n",
    "        # 3D IoU (via box3d_overlap from pytorch 3d)\n",
    "        pred_boxes = convert_to_box3d(preds['center'], preds['dims'], pred_yaw_vec)\n",
    "        gt_boxes = convert_to_box3d(targets['center'], targets['dims'], gt_yaw_vec)\n",
    "        try:\n",
    "            _, ious = box3d_overlap(pred_boxes, gt_boxes)\n",
    "            valid_ious = ious[~torch.isnan(ious)]\n",
    "            if len(valid_ious) > 0:\n",
    "                self.iou_3d(valid_ious.mean(), torch.tensor(0.0))\n",
    "            else:\n",
    "                logging.warning(f\"[WARNING {self.stage}] All IoUs are NaN!\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[ERROR {self.stage}] IoU calculation failed: {str(e)}\")\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {\n",
    "            'center_mae': self.mae_center.compute().item(),\n",
    "            'center_mse': self.mse_center.compute().item(),\n",
    "            'dims_mae': self.mae_dims.compute().item(),\n",
    "            'dims_mse': self.mse_dims.compute().item(),\n",
    "            'angle_error': self.angle_error.compute().item(),\n",
    "            'iou_3d': self.iou_3d.compute().item()\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.mae_center.reset()\n",
    "        self.mse_center.reset()\n",
    "        self.mae_dims.reset()\n",
    "        self.mse_dims.reset()\n",
    "        self.angle_error.reset()\n",
    "        self.iou_3d.reset()\n",
    "\n",
    "\n",
    "# Box Conversion Helpers\n",
    "def multibin_to_yaw(pred_cls, pred_reg, num_bins):\n",
    "    \"\"\"\n",
    "    Convert multi-bin predictions into a continuous yaw angle(in radians).\n",
    "    \"\"\"\n",
    "    bin_idx = torch.argmax(pred_cls, dim=1)\n",
    "    bin_size = 2 * math.pi / num_bins\n",
    "    bin_center = bin_idx.float() * bin_size + (bin_size / 2.0) - math.pi\n",
    "    pred_residual = pred_reg.gather(1, bin_idx.unsqueeze(1)).squeeze(1)\n",
    "    pred_angle = bin_center + pred_residual\n",
    "    return pred_angle\n",
    "\n",
    "def convert_to_box3d(centers, dims, orientations):\n",
    "    \"\"\"\n",
    "    Convert center, dimensions, and orientation (sinθ, cosθ) to 8-corner 3D boxes.\n",
    "    \"\"\"\n",
    "    dims = torch.clamp(dims, min=0.05)\n",
    "    batch_size = centers.shape[0]\n",
    "    device = centers.device\n",
    "    angles = torch.atan2(orientations[:, 0], orientations[:, 1])\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    rot_z = torch.zeros((batch_size, 3, 3), device=device)\n",
    "    rot_z[:, 0, 0] = cos\n",
    "    rot_z[:, 0, 1] = -sin\n",
    "    rot_z[:, 1, 0] = sin\n",
    "    rot_z[:, 1, 1] = cos\n",
    "    rot_z[:, 2, 2] = 1\n",
    "    lwh = dims / 2\n",
    "    corners = torch.tensor([\n",
    "        [-1, -1, -1], [1, -1, -1], [1, 1, -1], [-1, 1, -1],\n",
    "        [-1, -1, 1], [1, -1, 1], [1, 1, 1], [-1, 1, 1]\n",
    "    ], device=device).float()\n",
    "    corners = corners[None] * lwh[:, None]\n",
    "    corners = torch.bmm(corners, rot_z)\n",
    "    boxes = corners + centers[:, None]\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e74a3-3ab0-460f-b699-e0e0fc1c8441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scenes: 100%|██████████| 139/139 [00:13<00:00, 10.25it/s]\n",
      "Processing Scenes: 100%|██████████| 30/30 [00:01<00:00, 22.78it/s]\n",
      "Processing Scenes: 100%|██████████| 31/31 [00:00<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "MultiModalBBoxPredictor                       [4, 2]                    1\n",
      "├─ResNetBackbone: 1-1                         [4, 512, 4, 4]            --\n",
      "│    └─Sequential: 2-1                        [4, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-1                       [4, 64, 64, 64]           (9,408)\n",
      "│    │    └─BatchNorm2d: 3-2                  [4, 64, 64, 64]           (128)\n",
      "│    │    └─ReLU: 3-3                         [4, 64, 64, 64]           --\n",
      "│    │    └─MaxPool2d: 3-4                    [4, 64, 32, 32]           --\n",
      "│    └─Sequential: 2-2                        [4, 64, 32, 32]           --\n",
      "│    │    └─BasicBlock: 3-5                   [4, 64, 32, 32]           (73,984)\n",
      "│    │    └─BasicBlock: 3-6                   [4, 64, 32, 32]           (73,984)\n",
      "│    └─Sequential: 2-3                        [4, 128, 16, 16]          --\n",
      "│    │    └─BasicBlock: 3-7                   [4, 128, 16, 16]          (230,144)\n",
      "│    │    └─BasicBlock: 3-8                   [4, 128, 16, 16]          (295,424)\n",
      "│    └─Sequential: 2-4                        [4, 256, 8, 8]            --\n",
      "│    │    └─BasicBlock: 3-9                   [4, 256, 8, 8]            (919,040)\n",
      "│    │    └─BasicBlock: 3-10                  [4, 256, 8, 8]            (1,180,672)\n",
      "│    └─Sequential: 2-5                        [4, 512, 4, 4]            --\n",
      "│    │    └─BasicBlock: 3-11                  [4, 512, 4, 4]            (3,673,088)\n",
      "│    │    └─BasicBlock: 3-12                  [4, 512, 4, 4]            (4,720,640)\n",
      "├─LayerNorm: 1-2                              [4, 16, 512]              1,024\n",
      "├─SimplePointNetPP: 1-3                       [512, 512]                --\n",
      "│    └─PointNetConv: 2-6                      [1024, 128]               --\n",
      "│    │    └─Sequential: 3-13                  [1024, 128]               17,664\n",
      "│    │    └─MaxAggregation: 3-14              [1024, 128]               --\n",
      "│    └─PointNetConv: 2-7                      [512, 512]                --\n",
      "│    │    └─Sequential: 3-15                  [512, 512]                165,888\n",
      "│    │    └─MaxAggregation: 3-16              [512, 512]                --\n",
      "├─LayerNorm: 1-4                              [4, 256, 512]             1,024\n",
      "├─CrossAttentionFusion: 1-5                   [4, 256, 512]             --\n",
      "│    └─Linear: 2-8                            [4, 16, 512]              262,656\n",
      "│    └─MultiheadAttention: 2-9                [4, 256, 512]             1,050,624\n",
      "├─Sequential: 1-6                             [4, 256]                  --\n",
      "│    └─Linear: 2-10                           [4, 512]                  524,800\n",
      "│    └─ReLU: 2-11                             [4, 512]                  --\n",
      "│    └─Dropout: 2-12                          [4, 512]                  --\n",
      "│    └─Linear: 2-13                           [4, 256]                  131,328\n",
      "│    └─ReLU: 2-14                             [4, 256]                  --\n",
      "├─Sequential: 1-7                             [4, 3]                    --\n",
      "│    └─Linear: 2-15                           [4, 128]                  32,896\n",
      "│    └─ReLU: 2-16                             [4, 128]                  --\n",
      "│    └─Dropout: 2-17                          [4, 128]                  --\n",
      "│    └─Linear: 2-18                           [4, 3]                    387\n",
      "├─Sequential: 1-8                             [4, 3]                    --\n",
      "│    └─Linear: 2-19                           [4, 128]                  32,896\n",
      "│    └─ReLU: 2-20                             [4, 128]                  --\n",
      "│    └─Dropout: 2-21                          [4, 128]                  --\n",
      "│    └─Linear: 2-22                           [4, 3]                    387\n",
      "│    └─Softplus: 2-23                         [4, 3]                    --\n",
      "├─Sequential: 1-9                             [4, 2]                    --\n",
      "│    └─Linear: 2-24                           [4, 128]                  32,896\n",
      "│    └─ReLU: 2-25                             [4, 128]                  --\n",
      "│    └─Dropout: 2-26                          [4, 128]                  --\n",
      "│    └─Linear: 2-27                           [4, 2]                    258\n",
      "├─Sequential: 1-10                            [4, 2]                    --\n",
      "│    └─Linear: 2-28                           [4, 128]                  32,896\n",
      "│    └─ReLU: 2-29                             [4, 128]                  --\n",
      "│    └─Dropout: 2-30                          [4, 128]                  --\n",
      "│    └─Linear: 2-31                           [4, 2]                    258\n",
      "===============================================================================================\n",
      "Total params: 13,464,395\n",
      "Trainable params: 2,287,883\n",
      "Non-trainable params: 11,176,512\n",
      "Total mult-adds (G): 2.48\n",
      "===============================================================================================\n",
      "Input size (MB): 0.81\n",
      "Forward/backward pass size (MB): 64.00\n",
      "Params size (MB): 49.66\n",
      "Estimated Total Size (MB): 114.47\n",
      "===============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# main script\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # for visualization/logging\n",
    "    debugger = DebugVisualizer()\n",
    "    # Data preprocessing & loading\n",
    "    loaders = get_loaders()\n",
    "    # Model creation\n",
    "    model = MultiModalBBoxPredictor()\n",
    "\n",
    "    #print model architecture\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    dummy_rgb = torch.randn(4, 3, 128, 128).to(device)\n",
    "    dummy_lidar = torch.randn(4, 512, 3).to(device)\n",
    "    model_summary = summary(model, input_data=[dummy_rgb, dummy_lidar])\n",
    "    print(model_summary)\n",
    "    logging.info(model_summary)\n",
    "    #initialize model trainer\n",
    "    trainer = BBoxTrainer(model, loaders['train'], loaders['val'], loaders['test'], patience=50) \n",
    "\n",
    "    # Model training   \n",
    "    Config.NUM_EPOCHS\n",
    "    trainer.train(Config.NUM_EPOCHS)\n",
    "\n",
    "    # Evaluate detailed metrics on the test set.\n",
    "    test_metrics, test_loss = trainer.evaluate_metrics(loaders['test'], stage='test')\n",
    "    # Save metrics\n",
    "    with open(\"test_metrics.json\", \"w\") as f:\n",
    "        json.dump(test_metrics, f, indent=2)\n",
    "    # Ensure logs are flushed before the script exits\n",
    "    logging.shutdown()\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), \"final_model.pth\")\n",
    "    logging.info(\"Training complete. Models and metrics saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfea54-f9a0-4fcd-9529-dd68e387eca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
